{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from Constants import Constants, specialTokenList, specialTokens\n",
    "from All_Models import SSCL, GatedCNN, SelfAttnModel\n",
    "from utils import getSampler\n",
    "from Trainer import Trainer\n",
    "from LoadData import loadingData\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "1. Try Larger Vocab size\n",
    "4. Do Tkinter\n",
    "\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class args(object):\n",
    "\n",
    "    # Data\n",
    "    \n",
    "    \n",
    "    dataset = [\"HSpam14\", \"Honeypot\"][1]\n",
    "    \n",
    "    usingWeightRandomSampling = True\n",
    "    vocab_size = 8000 # if we create the new vocab size, we have to do the new preprocess again\n",
    "    full_data = True\n",
    "    validation_portion = 0.05\n",
    "    test_portion = 0.04\n",
    "    \n",
    "    \n",
    "    pickle_name = \"FullPickleData\"+ str(vocab_size) + \"Vocab.txt\"\n",
    "    pickle_name_beforeMapToIdx = \"FullPickleDatabeforeMapToIdx.txt\"\n",
    "\n",
    "    dataset_path = \"\"  # load a dataset and setting\n",
    "\n",
    "    ##### Arch\n",
    "        \n",
    "    usingPretrainedEmbedding = False\n",
    "    if usingPretrainedEmbedding:\n",
    "        embedding_dim = 300\n",
    "    else:\n",
    "        embedding_dim = 512\n",
    "\n",
    "    ## GatedCNN arch\n",
    "\n",
    "    GatedCNN_embedingDim = 128\n",
    "    GatedCNN_convDim = 64\n",
    "    GatedCNN_kernel = 3\n",
    "    GatedCNN_stride = 1\n",
    "    GatedCNN_pad = 1\n",
    "    GatedCNN_layers = 8\n",
    "    GatedCNN_dropout = 0.1\n",
    "        \n",
    "    ## SSCL arch\n",
    "\n",
    "    RNN_hidden = 256\n",
    "    num_CNN_filter = 256\n",
    "    CNN_kernel_size = 5\n",
    "    LSTM_dropout = 0.1\n",
    "    num_LSTM_layers = 1\n",
    "    SSCL_CNN_dropout = 0.1 \n",
    "    \n",
    "    ## Attn arch\n",
    "    \n",
    "\n",
    "    attnLenMaxSeq = 280 # Default, will be changed Later\n",
    "\n",
    "    # These Two has to be the same\n",
    "    attnWordVecDim = 128\n",
    "    attnModelDim = 128\n",
    "    \n",
    "    attnFFInnerDim = 256\n",
    "    attnNumLayers = 3\n",
    "    attnNumHead = 4\n",
    "    attnKDim = 64\n",
    "    attnVDim = 64\n",
    "    attnDropout = 0.1\n",
    "    \n",
    "    # Training params\n",
    "\n",
    "    confusion_matrics = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    L2 = 0.1\n",
    "    threshold = 0.5\n",
    "    lr = 0.002\n",
    "    n_epoch = 50\n",
    "\n",
    "    # If using Adam\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    adam_weight_decay = 0.01\n",
    "    \n",
    "    \n",
    "    earlyStopStep = 5000 # Set None if we don't want it\n",
    "    earlyStopEpoch = 1 #\n",
    "\n",
    "    # Logging the Training\n",
    "    val_freq = 50\n",
    "    val_steps = 3\n",
    "    log_freq = 10\n",
    "    model_save_freq = 1\n",
    "    model_name = 'GatedCNN_Vocab8000_RandomWeightedSampling_WithDropout'\n",
    "    model_path = './'+ dataset +'_Log/' + model_name + '/Model/'\n",
    "    log_path = './' + dataset +'_Log/' + model_name + '/Log/'\n",
    "\n",
    "args.device = device\n",
    "\n",
    "# Create the path for saving model and the log\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "if not os.path.exists(args.log_path):\n",
    "    os.makedirs(args.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Origin Data and do the Proprocessing\n",
      "Loading Honeypot dataset\n",
      "Data Splitation\n",
      "Number of Training Data:  894927\n",
      "Number of Validation Data:  45217\n",
      "Number of Test Data:  1885\n",
      "Preprocessing X_train\n",
      "Preprocessing X_validation\n",
      "Preprocessing X_test\n",
      "Generating text\n",
      "Original Vocab Size:  8080956\n",
      "The Pickle Data beforeMapToIdx Dumped to: Honeypot/FullPickleDatabeforeMapToIdx.txt\n",
      "Generating Datasets\n",
      "Training set map to Idx\n",
      "Validation set map to Idx\n"
     ]
    }
   ],
   "source": [
    "training_dataset, validation_dataset, test_dataset, text = loadingData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it after the training set\n",
    "args.numberOfSpammer = sum([t[-1] for t in training_dataset])\n",
    "args.numberOfNoSpammer = len(training_dataset)-args.numberOfSpammer\n",
    "args.len_max_seq = training_dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.usingWeightRandomSampling:\n",
    "    sampler = getSampler(training_dataset)\n",
    "else:\n",
    "    sampler = None\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False, sampler = sampler)\n",
    "valid_loader = DataLoader(\n",
    "    validation_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "trainer = Trainer(GatedCNN, args).to(device)\n",
    "\n",
    "print(\"Number of Parameters in this Model: \",trainer.num_all_params())\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(trainer.optim, 2000, gamma=0.85)\n",
    "# trainer.optim.param_groups[0]['lr']=\n",
    "allStep = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Structure: \\n\", trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while epoch < args.n_epoch:\n",
    "    for i, (texts, X , X_len, y) in enumerate(train_loader):\n",
    "        \n",
    "        trainer.train()\n",
    "        X, X_len, y = X.to(device), X_len.to(device), y.to(device)\n",
    "        \n",
    "        if trainer.optim.param_groups[0]['lr'] >= 0.00001:\n",
    "            scheduler.step()\n",
    "        start_t = time.time()\n",
    "#         trainer.train_step((X, X_len), y)\n",
    "        trainer.train_step(X, y)\n",
    "\n",
    "        end_t = time.time()\n",
    "        allStep += 1\n",
    "        print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "              (epoch, allStep, trainer.optim.param_groups[0]['lr'], trainer.loss.item(), trainer.accuracy.item(),\n",
    "               end_t - start_t))\n",
    "\n",
    "#         if trainer.accuracy.item() > 0.95: # Stop early\n",
    "#             raise StopIteration\n",
    "        if allStep % args.log_freq == 0:\n",
    "            trainer.plot_train_hist(args.model_name)\n",
    "            \n",
    "        \n",
    "        if args.earlyStopStep:\n",
    "            if allStep >= args.earlyStopStep:\n",
    "                    raise StopIteration\n",
    "        \n",
    "\n",
    "        if allStep % args.val_freq == 0:\n",
    "\n",
    "            for _ in range(args.val_steps):\n",
    "                trainer.eval()\n",
    "                stIdx = np.random.randint(\n",
    "                    0, len(validation_dataset) - args.batch_size)\n",
    "                v_text, v_X, v_X_len, v_y = validation_dataset[stIdx: stIdx +\n",
    "                                                       args.batch_size]\n",
    "                v_X, v_X_len, v_y = v_X.to(\n",
    "                    device), v_X_len.to(device), v_y.to(device)\n",
    "                start_t = time.time()\n",
    "#                 trainer.test_step((v_X, v_X_len), v_y)\n",
    "                trainer.test_step(v_X, v_y)\n",
    "                end_t = time.time()\n",
    "                print('| Epoch [%d] | Validation | Step [%d] |  Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\n",
    "                      (epoch, allStep, trainer.loss.item(), trainer.accuracy.item(), end_t - start_t))\n",
    "            trainer.calculateAverage()\n",
    "            clear_output()\n",
    "            print(\"TrainConfusion Matrix: \\n\")\n",
    "            display(pd.DataFrame(trainer.cms['Train'][-1]))\n",
    "            print(\"ValConfusion Matrix: \\n\")\n",
    "            display(pd.DataFrame(trainer.cms['Val'][-1]))\n",
    "            trainer.plot_all(args.model_name)\n",
    "            \n",
    "            \n",
    "            \n",
    "     # After every Epoch, if can be moved\n",
    "\n",
    "    epoch += 1\n",
    "    trainer.model_save(epoch)\n",
    "\n",
    "\n",
    "    if args.earlyStopEpoch:\n",
    "        if epoch >= args.earlyStopEpoch:\n",
    "            raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text, test_X, test_X_len, test_y  =  zip(test_dataset[0:])\n",
    "test_text, test_X, test_X_len, test_y = test_text[0], test_X[0].to(device), test_X_len[0].to(device), test_y[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, cm = trainer.test_step(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"The Test Loss: \", loss.item())\n",
    "print(\"The Test Accuracy: \", accuracy.item())\n",
    "print(\"Test Confusion Matrix: \\n\", cm)\n",
    "\n",
    "## Need Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(os.path.join(args.dataset, \"FullDataFromSQLHSpam14.html\"))[\n",
    "                    0].iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'maliciousMark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
